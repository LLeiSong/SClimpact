---
title: "Prepare occurrences, range maps and environmental variables"
author: "Lei Song"
date: "2024-09-17"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
library(here)
library(kableExtra)
library(ggplot2)
# devtools::install_github("davidsjoberg/ggsankey")
library(ggsankey)
library(dplyr)
library(circlize)
```

## Occurrences

Due to various considerations, we decided to use the occurrence records hosted on [GBIF](https://www.gbif.org) as the reference for species distribution modeling (SDM). However, recognizing the known issues and limitations in these datasets, we applied several data filtering steps to ensure the quality and reliability of the records. The details of these filtering steps will be introduced progressively.

### Data query

Since our focus is solely on mammalian species, we query occurrence records specifically for mammals. The `query_gbif` function can be utilized to efficiently retrieve a large amount of data from [GBIF](https://www.gbif.org).

```{r query_gbif}
# Query occurrences for all mammalian species from GBIF.
occ_get <- query_gbif(taxon = "Mammals", occ_dir = here("data/occurrences"))
```

### Data filter
#### Internal filtering during querying

[GBIF API](https://techdocs.gbif.org/en/openapi/) now allows users to apply filters when querying data, which can help refine searches and exclude unwanted data. In our case, when using the function `query_gbif`, we applied a few meta filters to remove records that are not relevant to our analysis. These filters include:

1. Zero coordinate: Coordinates are exactly (0,0). null island.
2. Country coordinate mismatch: The coordinates fall outside of the given country’s polygon.
3. Coordinate invalid: GBIF is unable to interpret the coordinates.
4. Coordinate out of range: The coordinates are outside of the range for decimal lat/lon values ((-90,90), (-180,180)).
5. Only keep PRESENT records.
6. Remove FOSSIL_SPECIMEN and LIVING_SPECIMEN.

#### Extra filtering after querying

Function `filter_gbif` applies a few steps (refers to GBIF data [blog1](https://data-blog.gbif.org/post/downloading-long-species-lists-on-gbif/) and [blog2](https://data-blog.gbif.org/post/gbif-filtering-guide/)) to filter the queried occurrences further. These filters include:

- Meta filter: 

  1. Remove old records (before 1900).
  2. Remove records with high uncertainty (coordinatePrecision >= 0.01 and  coordinateUncertaintyInMeters >= 10000).
  3. Remove possible GeoLocate centroids for coordinateUncertaintyInMeters (301,3036,999,9999).
  4. Remove points plotted along the prime meridian or equator (decimalLatitude == 0 or decimalLongitude == 0).
  5. Remove possible remove country centroids within 2km, capitals centroids within 2km, zoo and herbaria within 2km, and points within the ocean using package `CoordinateCleaner`.
  6. Remove the exact duplicated records.
  
- Filter spatial outliers: We only retain occurrences that fall within the [AOH maps](https://doi.org/10.1038/s41597-022-01838-w) of each species.
- Filter environmental outliers and spatial thinning: This step is not applied in the `filter_gbif` function but is instead incorporated within the modeling workflow, as it requires access to the environmental layers.

```{r filter_gbif}
# Query AOH for mammals
query_aoh(taxon = "Mammals", aoh_dir = here("data/IUCN_AOH_100m"))

# Filter the queried occurrences.
filter_gbif(x = occ_get, occ_dir = here("data/occurrences"),
            aoh_dir = here("data/IUCN_AOH_100m/Mammals"))
```

### Prepare CSVs and separate range maps

One key challenge in this step is matching the scientific names across different versions of the [Mammal Diversity Database (MDD)](https://zenodo.org/records/6407053).  In this process, we cross-match the scientific names between occurrences and range maps, then separate the occurrences into individual CSV files and the range maps into GEOJSON files.

```{r}
# Set some paths and directories
range_path <- here("data/IUCN/MAMMALS/MAMMALS.shp")
occ_path <- here("data/occurrences/occurrences_clean_0018860-240906103802322.csv")
sp_catalog_path <- here("data/occurrences/species_catalog_0018860-240906103802322.csv")
occ_dir <- here("data/occurrences/CSVs")
range_dir <- here("data/IUCN/Expert_Maps")

# Call the function
prepare_range(range_path, occ_path, sp_catalog_path, occ_dir, range_dir)
```

After this step, there are 2480 species left in the analysis.

## Environmental layers
### Data sources

We used bioclimatic variables (BIO 1-19) from the periods 1981–2010, 2011–2040, 2041–2070, and 2071–2100, sourced from [CHELSA Bioclim](https://chelsa-climate.org/bioclim/). CHELSA provides data across five models and three scenarios:

**Models:**

- GFDL-ESM4
- UKESM1-0-LL
- MPI-ESM1-2-HR
- IPSL-CM6A-LR
- MRI-ESM2-0

**Scenarios:**

- SSP 126
- SSP 370
- SSP 585

We also used the projected future LULC dataset made by [Chen et al. (2022)](https://www.nature.com/articles/s41597-022-01208-6), which provides land use and land cover maps from 2015 to 2100 at 5-year intervals under various SSP-RCP scenarios.

**Land classification:**

- 1 - Water
- 2 - Forest
- 3 - Grassland
- 4 - Barren
- 5 - Cropland
- 6 - Urban
- 7 - Permanent snow and ice

**Scenarios:**

- SSP1_RCP19
- SSP1_RCP26
- SSP2_RCP45
- SSP3_RCP70
- SSP4_RCP34
- SSP4_RCP60
- SSP5_RCP34
- SSP5_RCP85

We selected SSP1_RCP26, SSP3_RCP70, and SSP5_RCP85 to match the corresponding climate variable scenarios. We paired the year 2015 with 1981–2010 as the base condition, 2040 with 2011–2040, 2070 with 2041–2070, and 2100 with 2071–2100.

### Processing

To highlight the global-scale signals and make the computation efficient, we conduct the analysis at the scale of 10 km with [Pseudo-Mercator projection (EPSG:3857)](https://epsg.io/3857). The global template is defined as following:

```{r template}
template <- rast(xmin = -20037508.34, xmax = 20037508.34, 
                 ymin = -20048966.1, ymax = 20048966.1,
                 crs = "EPSG:3857", resolution = 10000)
```

The bioclimatic variables were calculated using the bilinear mean, while land cover was calculated based on the coverage of associated land cover types. To simplify the analysis and reduce potential bias from rare land cover types, we aggregated several categories: Forest to represent forest change (note that the original land cover map includes shrubs under forest), Grassland to represent low vegetation change, and Cropland and Urban to represent human impact.

The environmental variables are prepared as following:

```{r query_env}
clim_dir <- '/Volumes/gazelle/CHELSA_V2/GLOBAL/climatologies'
lc_dir <- file.path(
    "/Volumes/gazelle/future_land_projection_chen/Global",
    " 7-land-types\ LULC\ projection\ dataset\ under\ SSPs-RCPs")
dst_dir <- "data/variables"
prepare_vars(clim_dir, lc_dir, dst_dir, FALSE)
```

## Environmental variable selection

To enhance model reliability, it is recommended to diagnose variables for each species individually. We incorporated this step into our workflow using Bayesian Additive Regression Trees (BARTs). We randomly selected an equal number of pseudo-absence points to pair with presence points and applied BART to iteratively reduce the variables. This process was repeated for 30 iterations, and the most frequently used variables were selected. Highly correlated variables (correlation coefficient > 0.7) were subsequently removed. The script can be executed both locally or remotely on servers as follows:

```{r}
library(pbmcapply)
species_list <- list.files("data/occurrences/CSVs")
species_list <- gsub(".csv", "", species_list)
species_list <- gsub("_", " ", species_list)

var_path <- "data/variables/Env/AllEnv.tif"
occ_dir <- "data/occurrences"
aoh_dir <- "data/IUCN_AOH_100m/Mammals"
dst_dir <- "data/variables/variable_list"
tmp_dir <- "data/tmp"

pbmclapply(species_list, function(sp){
    var_selection(sp, var_path, occ_dir, aoh_dir, dst_dir, tmp_dir, 123)
}, mc.cores = 10)
```

We ran this step on HPC (see details in `tools/slurm_submit_vs.R`):

```{r}
# slurm submission
root_dir <- "/home/lsong/SCImpact/data"
species_list <- list.files(file.path(root_dir, "occurrences/CSVs"))
species_list <- gsub(".csv", "", species_list)
fnames <- list.files(file.path(root_dir, "variables/variable_list"))
fnames_to_move <- list.files(file.path(root_dir, "variables/variable_list"),
                             pattern = "allruns.csv")
fnames <- setdiff(fnames, fnames_to_move)
fnames <- gsub(".csv", "", fnames)
species_list <- setdiff(species_list, fnames)

chunk_length <- 8
species_list <- split(
    species_list, ceiling(seq_along(species_list) / chunk_length))

for (i in 1:length(species_list)){
    sp <- species_list[[i]]
    sp <- paste(sp, collapse = ",")
    print(sp)
    system(sprintf("sbatch schedulers/select_variable.sh %s", sp))
}
```

## Environmental subsampling occurrences and sampling background samples

Several studies have demonstrated that subsampling occurrences across environmental space can improve modeling accuracy and stability. Therefore, we included this step in our workflow as well. For background samples, we first extracted a large number (60000) of [**target-group-background (TGB)**](https://rvalavi.github.io/SDMwithRFs/), then environmentally thinned them.

```{r}
root_dir <- "/home/lsong/SCImpact"
src_dir <- file.path(root_dir, "data/occurrences/CSVs")
var_dir <- file.path(root_dir, "data/variables")
range_dir <- file.path(root_dir, "data/IUCN/Expert_Maps")
region_dir <- file.path(root_dir, "data/terr-ecoregions-TNC")
occ_dir <- file.path(root_dir, "data/occurrences/CSVs_thin")
bg_dir <- file.path(root_dir, "data/occurrences/bg")

env_sampling(sp_list, src_dir, var_dir, range_dir, 
             region_dir, occ_dir, bg_dir, 60000, 123, 30)
```

We also ran this step on HPC to save time (see details in `tools/slurm_submit_es.R`):

```{r}
# slurm submission
root_dir <- "/home/lsong/SCImpact/data"

species <- list.files(file.path(root_dir, "occurrences/CSVs"))
species <- gsub(".csv", "", species)
species2 <- list.files(file.path(root_dir, "variables", "variable_list"))
species2 <- gsub(".csv", "", species2)
species <- intersect(species, species2)

message(sprintf("%s species to process.", length(species)))

chunk_length <- 100
species_list <- split(
    species, ceiling(seq_along(species) / chunk_length))

for (i in 1:length(species_list)){
    sp <- species_list[[i]]
    sp <- paste(sp, collapse = ",")
    system(sprintf("sbatch schedulers/env_sampling.sh %s", sp))
}

# When all species are done, run this:
fnames <- list.files(file.path(root_dir, "occurrences", "CSVs_thin"), 
                     full.names = TRUE)
## Get the numbers
nums <- lapply(fnames, function(fname){
    occ <- read.csv(fname)
    bg <- read.csv(gsub("CSVs_thin", "bg", fname))
    data.frame(species = gsub(".csv", "", basename(fname)),
               num_occ = nrow(occ),
               num_bg = nrow(bg))
}) %>% bind_rows()

## Only keep the species with >=20 occurrences
nums <- nums %>% filter(num_occ >= 20)
fname <- file.path(root_dir, "occurrences", "species_qualified_sdm.csv")
write.csv(nums, fname, row.names = FALSE)
```

After completing these steps, the occurrence and background samples are ready for modeling. We are confident that the occurrences and environmental variables have been thoroughly examined and properly processed. Check `species_distribution_modeling.Rmd` for the details of modeling.

## Check the data

```{r}
library(ggplot2)

ggplot(nums, aes(x = log(num_occ), y = num_bg / 10000)) + 
  geom_point() +
    xlab("log(No. of occurrences)") +
    ylab("No. of background samples (10^4)") +
    geom_smooth(se = FALSE, color = "red") +
    theme_alluvial() +
    theme(axis.text = element_text(size = 12, color = "black"),
          axis.title = element_text(size = 14, color = "black"))
```

